{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Behavioral Cloning** \n",
    "\n",
    "---\n",
    "\n",
    "**Behavioral Cloning Project**\n",
    "\n",
    "The goals / steps of this project are the following:\n",
    "* Use the simulator to collect data of good driving behavior\n",
    "* Build, a convolution neural network in Keras that predicts steering angles from images\n",
    "* Train and validate the model with a training and validation set\n",
    "* Test that the model successfully drives around track one without leaving the road\n",
    "* Summarize the results with a written report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "import libraries\n",
    "\"\"\"\n",
    "import csv\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Lambda, Dense, Convolution2D, MaxPooling2D, Dropout, Activation, Flatten, Cropping2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "My Hyperparameters\n",
      "------------------------------\n",
      "test_size            := 0.2\n",
      "bool_udacity_data    := True\n",
      "nb_epoch             := 5\n",
      "keep_prob            := 0.7\n",
      "use_left_data        := True\n",
      "bool_my_data_2       := True\n",
      "use_right_data       := True\n",
      "debug_mode           := True\n",
      "img_dir              := ['../data/Udacity/', '../data/T1_Regular/IMG/', '../data/T1_Counter/IMG/']\n",
      "batch_size           := 64\n",
      "learning_rate        := 0.001\n",
      "steer_corr           := 0.2\n",
      "bool_my_data_1       := True\n",
      "csv_path             := ['../data/Udacity/driving_log.csv', '../data/T1_Regular/driving_log.csv', '../data/T1_Counter/driving_log.csv']\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "args = {\n",
    "    'test_size': 0.2,\n",
    "    'keep_prob': 0.7,\n",
    "    'nb_epoch': 5,\n",
    "    'batch_size': 64,\n",
    "    'learning_rate': 1.0e-3,\n",
    "    \n",
    "    'debug_mode':True,\n",
    "    \n",
    "    'bool_udacity_data': True,\n",
    "    'bool_my_data_1': True,\n",
    "    'bool_my_data_2': True,\n",
    "    'csv_path': [\n",
    "        '../data/Udacity/driving_log.csv',\n",
    "        '../data/T1_Regular/driving_log.csv',\n",
    "        '../data/T1_Counter/driving_log.csv'\n",
    "    ],\n",
    "    'img_dir': [\n",
    "        '../data/Udacity/',\n",
    "        '../data/T1_Regular/IMG/',\n",
    "        '../data/T1_Counter/IMG/'\n",
    "    ],\n",
    "    \n",
    "    'steer_corr': 0.2,\n",
    "    'use_left_data': True,\n",
    "    'use_right_data': True\n",
    "}\n",
    "\n",
    "print('-' * 30)\n",
    "print('My Hyperparameters')\n",
    "print('-' * 30)\n",
    "for key, value in args.items():\n",
    "    print('{:<20} := {}'.format(key, value))\n",
    "print('-' * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Load image paths and steering data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(args):\n",
    "    \"\"\"\n",
    "    Function: Load training data\n",
    "    Input:\n",
    "        args: input parameters saved in the type of parser.parse_args\n",
    "    Output:\n",
    "        image_paths: \n",
    "        labels:\n",
    "    \"\"\"\n",
    "\n",
    "    if args['debug_mode'] is True:\n",
    "        print(\"LOADING DATA......\")\n",
    "        \n",
    "    image_paths = list()\n",
    "    labels = list()\n",
    "\n",
    "    steer_corr = args['steer_corr']\n",
    "    use_left_data = args['use_left_data']\n",
    "    use_right_data = args['use_right_data']\n",
    "\n",
    "    bool_udacity_data = args['bool_udacity_data']\n",
    "    bool_my_data_1 = args['bool_my_data_1']\n",
    "    bool_my_data_2 = args['bool_my_data_2']\n",
    "    data_to_use = [bool_udacity_data, bool_my_data_1, bool_my_data_2]\n",
    "    csv_path = args['csv_path']\n",
    "    img_dir = args['img_dir']\n",
    "\n",
    "    for idx in range(len(data_to_use)):\n",
    "        if not data_to_use[idx]:\n",
    "            print('not using dataset', idx)\n",
    "            continue\n",
    "\n",
    "        lines = list()\n",
    "        # Import data from csv file\n",
    "        with open(csv_path[idx], newline='') as csvfile:\n",
    "            reader = csv.reader(csvfile, skipinitialspace=True, delimiter=',', quoting=csv.QUOTE_NONE)\n",
    "            for line in reader:\n",
    "                lines.append(line)\n",
    "\n",
    "            # print(\"The number of rows in dataset {} is {}\".format(idx, len(lines)-1))\n",
    "\n",
    "        for line in lines[1:]:\n",
    "            # skip the data with ~0 speed\n",
    "            if float(line[6]) < 0.1 :\n",
    "                continue\n",
    "\n",
    "            # center data\n",
    "            img_path_center = img_dir[idx] + line[0].split('\\\\')[-1]\n",
    "            image_paths.append(img_path_center)\n",
    "            label_center = float(line[3])\n",
    "            labels.append(label_center)\n",
    "\n",
    "            # left data\n",
    "            if use_left_data:\n",
    "                img_path_left = img_dir[idx] + line[1].split('\\\\')[-1]\n",
    "                image_paths.append(img_path_left)\n",
    "                label_left = float(line[3]) + steer_corr\n",
    "                labels.append(label_left)\n",
    "\n",
    "            # right data\n",
    "            if use_right_data:\n",
    "                img_path_left = img_dir[idx] + line[2].split('\\\\')[-1]\n",
    "                image_paths.append(img_path_left)\n",
    "                label_left = float(line[3]) - steer_corr\n",
    "                labels.append(label_left)\n",
    "\n",
    "    image_paths = np.array(image_paths)\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    return image_paths, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADING DATA......\n",
      "Total images: (64167,) \n",
      "Total labels: (64167,) \n"
     ]
    }
   ],
   "source": [
    "image_paths_ori, labels_ori = load_data(args)\n",
    "print(\"Total images: {} \".format(image_paths_ori.shape))\n",
    "print(\"Total labels: {} \".format(labels_ori.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Uniform the data\n",
    "Thanks to the useful tips from [Jeremy Shannon](https://github.com/jeremy-shannon/CarND-Behavioral-Cloning-Project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniform_data(image_paths_ori, labels_ori, display_mode = False):\n",
    "    \"\"\"\n",
    "    Function: Unifrom the data\n",
    "    Input:\n",
    "        image_paths_ori:\n",
    "        labels_ori:\n",
    "    Output:\n",
    "        image_paths: \n",
    "        labels:\n",
    "    \"\"\"\n",
    "    image_paths = np.copy(image_paths_ori)\n",
    "    labels = np.copy(labels_ori)\n",
    "    \n",
    "    num_bins = 23\n",
    "    avg_samples_per_bin = len(labels_ori)/num_bins\n",
    "    \n",
    "    hist, bins = np.histogram(labels_ori, num_bins)\n",
    "    \n",
    "    # uniform the data by shrinking those over the average \n",
    "    keep_probs = []\n",
    "    target = avg_samples_per_bin * 1.0\n",
    "    for i in range(num_bins):\n",
    "        if hist[i] < target:\n",
    "            keep_probs.append(1.)\n",
    "        else:\n",
    "            keep_probs.append(1./(hist[i]/target))\n",
    "    remove_list = []\n",
    "    for i in range(len(labels)):\n",
    "        for j in range(num_bins):\n",
    "            if labels[i] > bins[j] and labels[i] <= bins[j+1]:\n",
    "                # delete from X and y with probability 1 - keep_probs[j]\n",
    "                if np.random.rand() > keep_probs[j]:\n",
    "                    remove_list.append(i)\n",
    "    image_paths = np.delete(image_paths, remove_list, axis=0)\n",
    "    labels = np.delete(labels, remove_list)\n",
    "    \n",
    "    if display_mode is True:\n",
    "        \n",
    "        width = 0.7 * (bins[1] - bins[0])\n",
    "        center = (bins[:-1] + bins[1:]) / 2\n",
    "        plt.bar(center, hist, align='center', width=width, color = 'b')\n",
    "        \n",
    "        hist, bins = np.histogram(labels, num_bins)\n",
    "        plt.bar(center, hist, align='center', width=width, color = 'r')\n",
    "        \n",
    "        plt.plot((np.min(labels), np.max(labels)), (avg_samples_per_bin, avg_samples_per_bin), 'k-')\n",
    "    \n",
    "    return image_paths, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images shrink from (64167,) to (22943,) after uniform process\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x21e2a779208>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_paths, labels = uniform_data(image_paths_ori, labels_ori, display_mode = True)\n",
    "print(\"Total images shrink from {} to {} after uniform process\".format(image_paths_ori.shape, image_paths.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: (22943,) (22943,)\n",
      "Training: (18354,) (18354,)\n",
      "Test: (4589,) (4589,)\n"
     ]
    }
   ],
   "source": [
    "image_paths_train, image_paths_test, labels_train, labels_test = train_test_split(image_paths,\n",
    "                                                                                  labels,\n",
    "                                                                                  test_size=args['test_size'],\n",
    "                                                                                  random_state=42)\n",
    "print('Total:', image_paths.shape, labels.shape)\n",
    "print('Training:', image_paths_train.shape, labels_train.shape)\n",
    "print('Test:', image_paths_test.shape, labels_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 (Just for Test) Select some data randomly for debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: (64,) (64,)\n",
      "Test: (16,) (16,)\n"
     ]
    }
   ],
   "source": [
    "# Comment them when running the whole set of data\n",
    "indices_train = np.random.randint(0, len(image_paths_train), 64)\n",
    "indices_test = np.random.randint(0, len(image_paths_test), 16)\n",
    "image_paths_train = image_paths_train[indices_train]\n",
    "labels_train = labels_train[indices_train]\n",
    "image_paths_test = image_paths_test[indices_test]\n",
    "labels_test = labels_test[indices_test]\n",
    "\n",
    "print('Training:', image_paths_train.shape, labels_train.shape)\n",
    "print('Test:', image_paths_test.shape, labels_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5 Load images with generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(X_path, y, batch_size=64):\n",
    "    \"\"\"\n",
    "    Object: define a generator to extract training data and test data\n",
    "    \n",
    "    Generators can be a great way to work with large amounts of data. Instead of storing \n",
    "    the preprocessed data in memory all at once, using a generator you can pull pieces of \n",
    "    the data and process them on the fly only when you need them, which is much more memory-efficient.\n",
    "    \n",
    "    A generator is like a coroutine, a process that can run separately from another main routine, \n",
    "    which makes it a useful Python function. Instead of using return, the generator uses yield, \n",
    "    which still returns the desired output values but saves the current values of all the generator's\n",
    "    variables. When the generator is called a second time it re-starts right after the yield statement, \n",
    "    with all its variables set to the same values as before.\n",
    "    \"\"\"\n",
    "    num_samples = len(X_path)    \n",
    "    \n",
    "    while True: # Loop forever so the generator never terminates\n",
    "        \n",
    "        X_path, y = shuffle(X_path, y)\n",
    "        for offset in range(0, num_samples, int(batch_size/2)):\n",
    "            # Half batch size is used, since images are flipped in the following codes\n",
    "            X_path_sub = X_path[offset:offset + int(batch_size/2)]\n",
    "            y_sub   = y[offset:offset + int(batch_size/2)]\n",
    "\n",
    "            images = []\n",
    "            for image_path in X_path_sub:\n",
    "                image = cv2.cvtColor(cv2.imread(image_path),cv2.COLOR_BGR2RGB)\n",
    "                image_aug = cv2.flip(image,1)\n",
    "                images.append(image)\n",
    "                images.append(image_aug)\n",
    "            labels = []\n",
    "            for label in y_sub:\n",
    "                labels.extend([float(label),float(label)*-1])\n",
    "\n",
    "            X_batch = np.array(images,dtype='float64')\n",
    "            y_batch = np.array(labels,dtype='float64')\n",
    "\n",
    "            yield shuffle(X_batch, y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = generator(image_paths_train, labels_train, batch_size=args['batch_size'])\n",
    "test_gen = generator(image_paths_test, labels_test, batch_size=args['batch_size'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(args):\n",
    "    \"\"\"\n",
    "    Function: Build a deep learning model\n",
    "    Input:\n",
    "        args: input parameters saved in the type of parser.parse_args\n",
    "    Output:\n",
    "    \"\"\"\n",
    "    if args['debug_mode'] is True:\n",
    "        print(\"BUILDING MODEL......\")\n",
    "        \n",
    "    model = Sequential()\n",
    "    \n",
    "    # Normalize\n",
    "    model.add(Lambda(lambda x: x/255.0-0.5, input_shape=(160,320,3)))\n",
    "    \n",
    "    # Add a layer to crop images\n",
    "    model.add(Cropping2D(cropping=((70,25), (0,0))))  # remaining size: 65,320,3\n",
    "    \n",
    "    # Add three 5x5 convolution layers\n",
    "    model.add(Convolution2D(24, 5, 5, activation='elu', subsample=(2, 2)))\n",
    "    model.add(Convolution2D(36, 5, 5, activation='elu', subsample=(2, 2)))\n",
    "    model.add(Convolution2D(48, 5, 5, activation='elu', subsample=(2, 2)))\n",
    "    \n",
    "    # Add two 3x3 convolution layers\n",
    "    model.add(Convolution2D(64, 3, 3, activation='elu'))\n",
    "    model.add(Convolution2D(64, 3, 3, activation='elu'))\n",
    "    \n",
    "    # Add a flatten layer\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    # Add a dropout to overcome overfitting\n",
    "    model.add(Dropout(args['keep_prob']))\n",
    "    \n",
    "    # Add three fully connected layers\n",
    "    model.add(Dense(100, activation='elu'))\n",
    "    model.add(Dense(50, activation='elu'))\n",
    "    model.add(Dense(10, activation='elu'))\n",
    "    \n",
    "    # Add a fully connected output layer\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    # Summary\n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BUILDING MODEL......\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lambda_1 (Lambda)            (None, 160, 320, 3)       0         \n",
      "_________________________________________________________________\n",
      "cropping2d_1 (Cropping2D)    (None, 65, 320, 3)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 31, 158, 24)       1824      \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 14, 77, 36)        21636     \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 5, 37, 48)         43248     \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 3, 35, 64)         27712     \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 1, 33, 64)         36928     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2112)              0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 2112)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               211300    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                510       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 348,219\n",
      "Trainable params: 348,219\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yipen\\Anaconda3\\envs\\carnd-term1\\lib\\site-packages\\ipykernel_launcher.py:20: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(24, (5, 5), activation=\"elu\", strides=(2, 2))`\n",
      "C:\\Users\\yipen\\Anaconda3\\envs\\carnd-term1\\lib\\site-packages\\ipykernel_launcher.py:21: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(36, (5, 5), activation=\"elu\", strides=(2, 2))`\n",
      "C:\\Users\\yipen\\Anaconda3\\envs\\carnd-term1\\lib\\site-packages\\ipykernel_launcher.py:22: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(48, (5, 5), activation=\"elu\", strides=(2, 2))`\n",
      "C:\\Users\\yipen\\Anaconda3\\envs\\carnd-term1\\lib\\site-packages\\ipykernel_launcher.py:25: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), activation=\"elu\")`\n",
      "C:\\Users\\yipen\\Anaconda3\\envs\\carnd-term1\\lib\\site-packages\\ipykernel_launcher.py:26: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), activation=\"elu\")`\n"
     ]
    }
   ],
   "source": [
    "model = build_model(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, args, train_gen, test_gen, image_paths_train, image_paths_test):\n",
    "    \"\"\"\n",
    "    Function: train the model\n",
    "    Input:\n",
    "        model: built model \n",
    "        args: input parameters saved in the type of parser.parse_args\n",
    "        data: traning and validation data\n",
    "    Output:\n",
    "    \"\"\"\n",
    "    if args['debug_mode'] is True:\n",
    "        print(\"Training MODEL......\")\n",
    "\n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "    \n",
    "    model.fit_generator(train_gen,\n",
    "                        validation_data=test_gen,\n",
    "                        samples_per_epoch=len(image_paths_train)*2, # 2 for flip\n",
    "                        nb_val_samples=len(image_paths_test)*2,  # 2 for flip\n",
    "                        nb_epoch=args['nb_epoch'])\n",
    "                        \n",
    "    \n",
    "    #checkpoint = ModelCheckpoint('model-{epoch:03d}.h5')\n",
    "    model.save('model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MODEL......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yipen\\Anaconda3\\envs\\carnd-term1\\lib\\site-packages\\ipykernel_launcher.py:19: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "C:\\Users\\yipen\\Anaconda3\\envs\\carnd-term1\\lib\\site-packages\\ipykernel_launcher.py:19: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<generator..., steps_per_epoch=128, epochs=5, validation_data=<generator..., validation_steps=32)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "128/128 [==============================] - 109s 854ms/step - loss: 0.0141 - val_loss: 0.0761\n",
      "Epoch 2/5\n",
      "128/128 [==============================] - 63s 491ms/step - loss: 0.0023 - val_loss: 0.0692\n",
      "Epoch 3/5\n",
      "128/128 [==============================] - 64s 496ms/step - loss: 0.0017 - val_loss: 0.0692\n",
      "Epoch 4/5\n",
      "128/128 [==============================] - 63s 488ms/step - loss: 0.0013 - val_loss: 0.0659\n",
      "Epoch 5/5\n",
      "128/128 [==============================] - 63s 490ms/step - loss: 0.0011 - val_loss: 0.0648\n",
      "DONE!\n"
     ]
    }
   ],
   "source": [
    "train_model(model, args, train_gen, test_gen, image_paths_train, image_paths_test)\n",
    "print(\"DONE!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
